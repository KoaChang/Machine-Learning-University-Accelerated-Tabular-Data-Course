{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Tabular Data - Lecture 1</a>\n",
    "\n",
    "## Final Project:  K Nearest Neighbors (KNN)\n",
    "\n",
    "Build a [K Nearest Neighbors Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) to predict the __label__ field (substitute or not substitute) of the Amazon product substitute dataset.\n",
    "\n",
    "### Final Project Problem: Product Substitute Prediction\n",
    "\n",
    "__Problem Definition__:\n",
    "Given a pair of products, (A, B), we say that B is a \"substitute\" for A if a customer would buy B in place of A -- say, if A were out of stock.\n",
    "\n",
    "The goal of this project is to predict a substitute relationship between pairs of products. Complete the tasks in this notebook and submit your model's predictions as a CSV file to the leaderboard: __https://mlu.corp.amazon.com/contests/redirect/35__\n",
    "\n",
    "1. <a href=\"#1\">Read the datasets</a> (Given) \n",
    "2. <a href=\"#2\">Data Processing</a> (Implement)\n",
    "    * <a href=\"#21\">Exploratory Data Analysis</a>\n",
    "    * <a href=\"#22\">Select features to build the model</a> (Suggested)\n",
    "    * <a href=\"#23\">Train - Validation - Test Datasets</a>\n",
    "    * <a href=\"#24\">Data Processing with Pipeline</a>\n",
    "3. <a href=\"#3\">Train (and Tune) a Classifier on the Training Dataset</a> (Implement)\n",
    "4. <a href=\"#3\">Make Predictions on the Test Dataset</a> (Implement)\n",
    "5. <a href=\"#4\">Write the Test Predictions to a CSV file</a> (Given)\n",
    "\n",
    "\n",
    "__Datasets and Files:__\n",
    "\n",
    "\n",
    "* __training.csv__: Training data with product pair features and corresponding labels:\n",
    "> - `ID:` ID of the record\n",
    "> - `label:` Tells whether the key and candidate products are substitutes (1) or not (0).\n",
    "> - `key_asin ...:` Key product ASIN features \n",
    "> - `cand_asin ...:` Candidate product ASIN features \n",
    "\n",
    "\n",
    "* __public_test_features.csv__: Test data with product pairs features __without__ labels:\n",
    "> - `ID:` ID of the record\n",
    "> - `key_asin ...:` Key product ASIN features \n",
    "> - `cand_asin ...:` Candidate product ASIN features \n",
    "\n",
    "\n",
    "* __metadata-dataset.xlsx__: Provides detailed information about all key_ and cand_ columns in the training and test sets. Try to select some useful features to include in the model, as not all of them are suitable. `|Region Id|MarketPlace Id|ASIN|Binding Code|binding_description|brand_code|case_pack_quantity|, ...`\n",
    "\n",
    "\n",
    "* __Sample submission file:__ You can see a sample file: sample-submission.csv under data/final_project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Read the datasets</a> (Given)\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we read the __training__ and __test__ datasets into dataframes, using [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html). This library allows us to read and manipulate our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "  \n",
    "training_data = pd.read_csv('../../data/final_project/training.csv')\n",
    "test_data = pd.read_csv('../../data/final_project/public_test_features.csv')\n",
    "\n",
    "print('The shape of the training dataset is:', training_data.shape)\n",
    "print('The shape of the test dataset is:', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Data Processing</a> (Implement)\n",
    "(<a href=\"#0\">Go to top</a>) \n",
    "\n",
    "### 2.1 <a name=\"21\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We look at number of rows, columns, and some simple statistics of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement more EDA here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 <a name=\"22\">Select features to build the model</a> \n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "For a quick start, we recommend using only a few of the numerical features for both key_ and cand_ ASINs: __item_package_quantity__, __item_height__, __item_width__, __item_length__, __item_weight__, __pkg_height__, __pkg_width__, __pkg_length__, __pkg_weight__. Feel free to explore other fields from the metadata-dataset.xlsx file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "numerical_features = [\"key_item_package_quantity\", \n",
    "                      \"key_item_height\", \"key_item_width\", \"key_item_length\", \"key_item_weight\", \n",
    "                      \"key_pkg_height\", \"key_pkg_width\", \"key_pkg_length\", \"key_pkg_weight\",\n",
    "                      \"cand_item_package_quantity\", \n",
    "                      \"cand_item_height\", \"cand_item_width\", \"cand_item_length\", \"cand_item_weight\", \n",
    "                      \"cand_pkg_height\", \"cand_pkg_width\", \"cand_pkg_length\", \"cand_pkg_weight\"]\n",
    "\n",
    "model_features = numerical_features\n",
    "model_target = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 <a name=\"23\">Train - Validation Datasets</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We already have training and test datasets, however the test dataset is missing the labels - the goal of the project is to predict these labels and submit them to leaderboard to get scored. \n",
    "\n",
    "To produce a validation set to evaluate model performance before submitting to the leaderboard, split the training dataset into train and validation subsets using sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. Validation data you get here will be used later in section 3 to tune your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 <a name=\"24\">Data processing with Pipeline</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "Build a [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)  to impute the missing values and scale the numerical features, and finally train a [K Nearest Neighbors Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)  on the imputed and scaled dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Train (and Tune) a Classifier</a> (Implement)\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Train and tune the [K Nearest Neighbors Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) pipeline. The __\"Model Tuning\"__ section of the __MLA-TAB-Lecture1-KNN.ipynb__ can be useful here. You can loop over different K values and check validation accuracy each time and select best performing K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Make Predictions on the Test Dataset</a> (Implement)\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Use the trained classifier to predict the labels on the test set. Test accuracy would be displayed upon a valid submission to the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "\n",
    "# Get test data to test the classifier\n",
    "# ! test data should come from public_test_features.csv !\n",
    "# ...\n",
    "\n",
    "# Use the trained model to make predictions on the test dataset\n",
    "# test_predictions = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Write the test predictions to a CSV file</a> (Given)\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Use the following code to write the test predictions to a CSV file. Download locally the CSV file from the SageMaker instance, and upload it to __https://mlu.corp.amazon.com/contests/redirect/35__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"ID\", \"label\"])\n",
    "result_df[\"ID\"] = test_data[\"ID\"].tolist()\n",
    "result_df[\"label\"] = test_predictions\n",
    "\n",
    "result_df.to_csv(\"../../data/project_day1_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Double-check submission file against the sample_submission.csv')\n",
    "sample_submission_df = pd.read_csv('../../data/final_project/sample-submission.csv')\n",
    "print('Differences between project_day1_result IDs and sample submission IDs:',(sample_submission_df['ID'] != result_df['ID']).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
